{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a86051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "PROJECT_ROOT= r\"H:\\Group-Activity-Recognition\"\n",
    "sys.path.append(os.path.abspath(PROJECT_ROOT))\n",
    "from DataLoader import Person , Group\n",
    "\n",
    "dataset_root = r\"H:\\Group-Activity-Recognition\\dataset\"\n",
    "annot_path =   f\"{dataset_root}/annot_all.pkl\"\n",
    "videos_path =  f\"{dataset_root}/videos\"\n",
    "\n",
    "people_activity_clases = [\"Waiting\", \"Setting\", \"Digging\", \"Falling\" ,\"Spiking\"\t, \"Blocking\", \"Jumping\"\t, \"Moving\", \"Standing\"]\n",
    "person_activity_labels  = {class_name.lower():i for i, class_name in enumerate(people_activity_clases)}\n",
    "\n",
    "group_activity_clases = [\"r_set\", \"r_spike\" , \"r-pass\", \"r_winpoint\", \"l_winpoint\", \"l-pass\", \"l-spike\", \"l_set\"]\n",
    "group_activity_labels  = {class_name:i for i, class_name in enumerate(group_activity_clases)}\n",
    "\n",
    "activities_labels = {\"person\": person_activity_labels, \"group\": group_activity_labels}\n",
    "\n",
    "train_spilt = [1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(224, 224),  \n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "data_loader = Person(videos_path, annot_path, split=train_spilt, seq=False, labels=person_activity_labels, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4434b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame , label = data_loader[0]\n",
    "\n",
    "label.shape # (,9) class of person activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame , label = data_loader[0]\n",
    "\n",
    "label.shape # (,9) class of person activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame , label = data_loader[50]\n",
    "\n",
    "label_idex = label.argmax().item()\n",
    "print(f\"{people_activity_clases[label_idex]}\")\n",
    "\n",
    "plt.figure(figsize=(2, 2)) \n",
    "plt.imshow(frame.permute(1,2,0))  # Converts from (C, H, W) to (H, W, C)\n",
    "plt.axis('off')  # Optional: to hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(224, 224),  \n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "data_loader = Person(videos_path, annot_path, split=train_spilt, seq=True, labels=person_activity_labels, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, label = data_loader[100]  \n",
    "\n",
    "label.shape # (12 player , 9 frame , label of 9 class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72336ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.shape #  (12 player, 9 frame, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f94a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idex = label[0, 0].argmax().item()\n",
    "print(f\"{people_activity_clases[label_idex]}\")\n",
    "\n",
    "plt.figure(figsize=(2, 2)) # frist player  - first frame\n",
    "plt.imshow(clip[0, 0].permute(1, 2, 0).numpy())  # Converts from (C, H, W) to (H, W, C)\n",
    "plt.axis('off')  # Optional: to hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a23dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(224, 224),  \n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "data_loader = Group(videos_path, annot_path, split=train_spilt, crops=False , seq=False, labels=group_activity_labels, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame , label = data_loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f282fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label.shape) # (,8)\n",
    "label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = data_loader[0][1].argmax().item()\n",
    "print(f\"{group_activity_clases[index]}\")\n",
    "\n",
    "plt.imshow(data_loader[0][0].permute(1,2,0)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(224, 224),  \n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "data_loader= Group(videos_path, annot_path, split=train_spilt, crops=True , seq=False, labels=group_activity_labels, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader) # the differents between case 1 and 2 the input consist of 12 bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3581e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_crops, label = data_loader[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label.shape) # (,8)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_crops.shape # (12, C, H, W) ---> 12 bbox of the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df35b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from models.baseline1.BaseLine1 import Group_Activity_Classifer\n",
    "\n",
    "\n",
    "from DataLoader import Group, group_activity_labels\n",
    "from eval_utils import get_f1_score , plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model\n",
    "\n",
    "\n",
    "Project_Root = r\"H:\\Group-Activity-Recognition\"\n",
    "Config_file_path = r\"H:\\Group-Activity-Recognition\\CVR16\\configs\\Baseline1.yml\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "\n",
    "def validation(model, val_loader, criterion, device, epoch, writer, logger, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            _, target_class = targets.max(1) \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(target_class).sum().item()\n",
    "            \n",
    "            y_true.extend(target_class.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    f1_score = get_f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    writer.add_scalar('Validation/F1Score', f1_score, epoch)\n",
    "    \n",
    "    fig = plot_confusion_matrix(y_true, y_pred, class_names)\n",
    "    writer.add_figure('Validation/ConfusionMatrix', fig, epoch)\n",
    "    \n",
    "    writer.add_scalar('Validation/Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Validation/Accuracy', accuracy, epoch)\n",
    "    \n",
    "    logger.info(f\"Epoch {epoch} | Valid Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}% | F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "    \n",
    "def train_model(Config_file_path):\n",
    "    config = load_config(Config_file_path)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    exp_dir = os.path.join(\n",
    "        f\"{Project_Root}/training/baseline1/{config[\"experiment\"]['output_dir']}\",\n",
    "        f\"{config[\"experiment\"]['name']}_V{config[\"experiment\"]['version']}_{timestamp}\"\n",
    "    )\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    logger = setup_logging(exp_dir)\n",
    "    logger.info(f\"Starting experiment: {config[\"experiment\"]['name']}_V{config[\"experiment\"]['version']}\")\n",
    "\n",
    "    writer = SummaryWriter(log_dir=os.path.join(exp_dir, 'tensorboard'))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    set_seed(config[\"experiment\"]['seed'])\n",
    "    logger.info(f\"Set random seed: {config[\"experiment\"]['seed']}\")\n",
    "    \n",
    "    train_transforms = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=(3, 7)),\n",
    "            A.ColorJitter(brightness=0.2),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.GaussNoise()\n",
    "        ], p=0.90),\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(),\n",
    "            A.VerticalFlip(),\n",
    "        ], p=0.05),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    val_transforms = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = Group(\n",
    "        videos_path=f\"{config[\"data\"]['videos_path']}\",\n",
    "        annot_path=f\"{config[\"data\"]['annot_path']}\",\n",
    "        split=config[\"data\"]['video_splits']['train'],\n",
    "        labels=group_activity_labels, \n",
    "        transform=train_transforms\n",
    "    )\n",
    "    \n",
    "    val_dataset = Group(\n",
    "        videos_path=f\"{config[\"data\"]['videos_path']}\",\n",
    "        annot_path=f\"{config[\"data\"]['annot_path']}\",\n",
    "        split=config[\"data\"]['video_splits']['validation'],\n",
    "        labels=group_activity_labels,\n",
    "        transform=val_transforms\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"training\"]['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"training\"]['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model = Group_Activity_Classifer(\n",
    "        num_classes=len(group_activity_labels),\n",
    "    )\n",
    "    model =  model.to(device)\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr= config[\"training\"]['learning_rate'],\n",
    "        weight_decay=config[\"training\"]['weight_decay']\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler() # for mixed precision training \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    ) # reduce the learning rate when the validation loss plateaus\n",
    "    config_save_path = os.path.join(exp_dir, 'config.yaml')\n",
    "    with open(config_save_path, 'w') as config_file:\n",
    "        yaml.dump(config, config_file)\n",
    "    logger.info(f\"Configuration saved to {config_save_path}\")\n",
    "    \n",
    "    logger.info(\"starting training...\")\n",
    "    \n",
    "    for epoch in range(config[\"training\"]['epochs']):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        model.train()\n",
    "        logger.info(f'epoch {epoch+1}/{config[\"training\"][\"epochs\"]}\\n')\n",
    "        \n",
    "        for batch_idx ,(images , labels) in enumerate(train_loader):\n",
    "            images = images.to(device) \n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_classes  = preds.argmax(dim=1)\n",
    "            true_classes = labels.argmax(dim=1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted_classes == true_classes).sum().item()\n",
    "            \n",
    "            if batch_idx %100 == 0:\n",
    "                logger.info(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f} - Accuracy: {total_correct/total_samples:.4f}\")\n",
    "    \n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_accuracy =  100. * total_correct / total_samples\n",
    "        logger.info(f\"Epoch {epoch+1}/{config[\"training\"][\"epochs\"]} - Loss: {avg_loss:.4f} - Accuracy: {avg_accuracy:.2f}%\")\n",
    "        \n",
    "        writer.add_scalar('loss/train' , avg_loss, epoch)\n",
    "        writer.add_scalar('accuracy/train' , avg_accuracy, epoch)\n",
    "        val_loss, val_acc = validation(model, val_loader, criterion, device, epoch, writer, logger, config.model['num_clases_label'])\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('Training/LearningRate', current_lr, epoch)\n",
    "        logger.info(f'Current learning rate: {current_lr}')\n",
    "        save_checkpoint_model(model, optimizer, epoch, val_acc, config, exp_dir)\n",
    "       \n",
    "    writer.close()\n",
    "    final_model_path = os.path.join(exp_dir, 'final_model.pth')\n",
    "    torch.save({\n",
    "        'epoch': config.training['epochs'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'config': config,\n",
    "    }, final_model_path)\n",
    "    \n",
    "    logger.info(f\"Training completed. Final model saved to: {final_model_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70edd557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 02:08:33,601 [INFO] Starting experiment: Baseline_B1_V1\n",
      "2025-04-26 02:08:33,611 [INFO] Using device: cpu\n",
      "2025-04-26 02:08:33,625 [INFO] Set random seed: 31\n",
      "2025-04-26 02:08:37,583 [INFO] Training dataset size: 19368\n",
      "2025-04-26 02:08:37,584 [INFO] Validation dataset size: 12069\n",
      "2025-04-26 02:08:38,453 [INFO] Configuration saved to H:\\Group-Activity-Recognition/training/baseline1/outputs\\Baseline_B1_V1_20250426_020833\\config.yaml\n",
      "2025-04-26 02:08:38,455 [INFO] starting training...\n",
      "2025-04-26 02:08:38,457 [INFO] epoch 1/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALkamaL\\AppData\\Local\\Temp\\ipykernel_11884\\1087055522.py:168: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # for mixed precision training\n",
      "c:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(Config_file_path)\n",
      "Cell \u001b[1;32mIn[3], line 190\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(Config_file_path)\u001b[0m\n\u001b[0;32m    187\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    188\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx ,(images , labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    191\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m    192\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ALkamaL\\anaconda3\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(Config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39933e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join('..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from models.LstmGroup import LSTMGroup\n",
    "from dataloader.DataLoader import Group, group_activity_labels\n",
    "from eval_utils import get_f1_score, plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model\n",
    "from baselines.trainer import Tranier\n",
    "\n",
    "\n",
    "ROOT = \"/kaggle/\"\n",
    "PROJECT_ROOT= \"/kaggle/working/Group-Activity-Recognition\"\n",
    "CONFIG_FILE_PATH = \"/kaggle/working/Group-Activity-Recognition/modeling/configs/Baseline B4.yml\"\n",
    "\n",
    "sys.path.append(os.path.abspath(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.DataLoader import Group, group_activity_labels\n",
    "from eval_utils import get_f1_score , plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model\n",
    "\n",
    "\n",
    "\n",
    "class LSTM_Trainer():\n",
    "    def __init__(self , config_file_path, project_root):\n",
    "        self.Project_Root = project_root\n",
    "        self.config =  load_config(config_file_path)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.exp_dir = os.path.join(\n",
    "            f\"{self.Project_Root}/training/baseline4/{self.config['experiment']['output_dir']}\",\n",
    "            f\"{self.config['experiment']['name']}_V{self.config['experiment']['version']}_{timestamp}\"\n",
    "        )\n",
    "        self.device = torch.device(\"cuuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        self.model  = LSTMGroup(\n",
    "            input_size=self.config['model']['input_size'],\n",
    "            hidden_size=self.config['model']['hidden_size'],\n",
    "            num_layers=self.config['model']['num_layers'],\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "        ).to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(),\n",
    "                        lr= self.config['training']['learning_rate'],\n",
    "                        weight_decay=self.config['training']['weight_decay'])\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=3, verbose=True )\n",
    "        self.scaler = GradScaler('cuda')\n",
    "        class_weights = self.calculate_class_weights(self.train_loader)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.logger = setup_logging(self.exp_dir)\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(self.exp_dir, 'tensorboard'))\n",
    "        self.train_loader, self.val_loader = self.prepare_data()\n",
    "        self.class_names = self.config['model']['num_classes_label']\n",
    "        config_save_path = os.path.join(self.exp_dir, 'config.yaml')\n",
    "        with open(config_save_path, 'w') as config_file:\n",
    "            yaml.dump(self.config, config_file)\n",
    "        self.logger.info(f\"Configuration saved to {config_save_path}\")\n",
    "    \n",
    "    def concat(self , batch):\n",
    "        clips , label = zip(*batch)\n",
    "        clips , label  = torch.stack(clips , dim =0) , torch.stack(label , dim =0)\n",
    "        labels = labels[:, -1, :]  \n",
    "        return clips, labels\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        train_transforms = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 7)),\n",
    "                A.ColorJitter(brightness=0.2),\n",
    "                A.RandomBrightnessContrast(),\n",
    "                A.GaussNoise()\n",
    "            ], p=0.80),\n",
    "            A.OneOf([A.HorizontalFlip(), A.VerticalFlip()], p=0.05),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        val_transforms = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        train_dataset = Group(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['train'],\n",
    "            labels=group_activity_labels,\n",
    "            transform=train_transforms\n",
    "        )\n",
    "\n",
    "        val_dataset = Group(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['validation'],\n",
    "            labels=group_activity_labels,\n",
    "            transform=val_transforms\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "        self.logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=True, num_workers=4, pin_memory=True\n",
    "            )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=False, num_workers=4, pin_memory=True\n",
    "            )\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def calculate_class_weights(self , train_loader):\n",
    "        labels = [label.max().item() for batch in train_loader for label in batch[1]]\n",
    "        class_counts = torch.bincount(torch.tensor(labels))\n",
    "        total_samples = len(labels)\n",
    "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        return class_weights.to(self.device)\n",
    "    \n",
    "        \n",
    "    def validate(self , epoch):\n",
    "        self.model.eval()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs , targets in self.val_loader:\n",
    "                inputs , targets  =  inputs.to(self.device) , targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                _, target_class = targets.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(target_class).sum().item()\n",
    "\n",
    "                y_true.extend(target_class.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        f1_score = get_f1_score(y_true, y_pred, average=\"weighted\")\n",
    "        self.writer.add_scalar('Validation/Loss', avg_loss, epoch)\n",
    "        self.writer.add_scalar('Validation/Accuracy', accuracy, epoch)\n",
    "        self.writer.add_scalar('Validation/F1Score', f1_score, epoch)\n",
    "        self.writer.add_figure('Validation/ConfusionMatrix', plot_confusion_matrix( y_true, y_pred, class_names = self.config[\"model\"]['num_classes_label'] , save_path = \"/kaggle/working/\"))\n",
    "\n",
    "        self.logger.info(f\"Epoch {epoch} | Valid Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}% | F1 Score: {f1_score:.4f}\")\n",
    "        return avg_loss, accuracy\n",
    "        \n",
    "    def train(self):\n",
    "        self.logger.info(\"Starting training...\")\n",
    "        \n",
    "        for epoch in range(self.config['training']['epochs']):\n",
    "            self.model.train()\n",
    "            total_loss, total_correct, total_samples = 0, 0, 0  \n",
    "            self.logger.info(f\"Epoch {epoch + 1}/{self.config['training']['epochs']}\")\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                with autocast(dtype=torch.float16):\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, labels)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted_classes = preds.argmax(dim=1)\n",
    "                true_classes = labels.argmax(dim=1)\n",
    "                total_samples += labels.size(0)\n",
    "                total_correct += (predicted_classes == true_classes).sum().item()\n",
    "                if batch_idx % 100 == 0:\n",
    "                    acc = total_correct / total_samples\n",
    "                    self.logger.info(f\"Batch {batch_idx}/{len(self.train_loader)} - Loss: {loss.item():.4f} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            avg_accuracy = 100. * total_correct / total_samples\n",
    "            self.writer.add_scalar('loss/train', avg_loss, epoch)\n",
    "            self.writer.add_scalar('accuracy/train', avg_accuracy, epoch)\n",
    "\n",
    "            self.logger.info(f\"Epoch {epoch + 1} Summary: Loss: {avg_loss:.4f} | Accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "            val_loss, val_acc = self.validate(epoch)\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar('Training/LearningRate', current_lr, epoch)\n",
    "            self.logger.info(f\"Current learning rate: {current_lr}\")\n",
    "\n",
    "            save_checkpoint_model(self.model, self.optimizer, epoch, val_acc,  self.exp_dir , self.config)\n",
    "\n",
    "        self.writer.close()\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        final_model_path = os.path.join(self.exp_dir, 'final_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': self.config['training']['epochs'],\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "        }, final_model_path)\n",
    "        self.logger.info(f\"Training completed. Final model saved to: {final_model_path}\")\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
