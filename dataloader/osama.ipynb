{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c059560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join('..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25323cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALkamaL\\anaconda3\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ClassiferNN' from 'models.PeronActivityClassifer' (i:\\Group-Activity-Recognition\\CVR16\\models\\PeronActivityClassifer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGroupActivityClassifer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Group_Activity_Classifer\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataloader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Group, group_activity_labels , Person , person_activity_labels\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_f1_score, plot_confusion_matrix\n",
      "File \u001b[1;32mi:\\Group-Activity-Recognition\\CVR16\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGroupActivityClassifer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Group_Activity_Classifer \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPeronActivityClassifer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassiferNN\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ClassiferNN' from 'models.PeronActivityClassifer' (i:\\Group-Activity-Recognition\\CVR16\\models\\PeronActivityClassifer.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from datetime import datetime\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from models.GroupActivityClassifer import Group_Activity_Classifer\n",
    "from dataloader.DataLoader import Group, group_activity_labels , Person , person_activity_labels\n",
    "from eval_utils import get_f1_score, plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model\n",
    "from baselines.trainer import Tranier\n",
    "class Person_Tranier(Tranier):\n",
    "    def __init__(self, config_file_path, project_root):\n",
    "        self.Project_Root = project_root\n",
    "        self.config = load_config(config_file_path)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.exp_dir = os.path.join(\n",
    "            f\"{self.Project_Root}/training/baseline3/{self.config['experiment']['output_dir']}\",\n",
    "            f\"{self.config['experiment']['name']}_V{self.config['experiment']['version']}_{timestamp}\"\n",
    "        )\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        self.set_seed(self.config['experiment']['seed'])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Group_Activity_Classifer(num_classes=len(person_activity_labels)).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(),\n",
    "                        lr= self.config['training']['learning_rate'],\n",
    "                        weight_decay=self.config['training']['weight_decay'])\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=3, verbose=True ) \n",
    "        self.scaler = GradScaler()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.logger = setup_logging(self.exp_dir)\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(self.exp_dir, 'tensorboard'))\n",
    "        self.train_loader, self.val_loader = self.prepare_data()\n",
    "        self.class_names = self.config['model']['num_clases_label']\n",
    "        config_save_path = os.path.join(self.exp_dir, 'config.yaml')\n",
    "        with open(config_save_path, 'w') as config_file:\n",
    "            yaml.dump(self.config, config_file)\n",
    "        self.logger.info(f\"Configuration saved to {config_save_path}\")\n",
    "        super(Tranier).__init__()\n",
    "    def prepare_data(self):\n",
    "        new_train_transforms = A.Compose([\n",
    "            A.Resize(256, 256),  \n",
    "            A.RandomRotate90(),  \n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        new_val_transforms = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        new_train_dataset = Person(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['train'],\n",
    "            labels=person_activity_labels,\n",
    "            transform=new_train_transforms\n",
    "        )\n",
    "\n",
    "        new_val_dataset = Person(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['validation'],\n",
    "            labels=person_activity_labels,\n",
    "            transform=new_val_transforms\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"New training dataset size: {len(new_train_dataset)}\")\n",
    "        self.logger.info(f\"New validation dataset size: {len(new_val_dataset)}\")\n",
    "\n",
    "        new_train_loader = DataLoader(\n",
    "            new_train_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        new_val_loader = DataLoader(\n",
    "            new_val_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        return new_train_loader, new_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef4323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \"__main__\":\\n    trainer = Trainer(\\n        config_file_path=r\"/kaggle/working/CVR16/configs/Baseline1.yml\",\\n        project_root=r\"/kaggle/working/CVR16\"\\n    )\\n    trainer.train()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if __name__ == \"__main__\":\n",
    "    trainer = Trainer(\n",
    "        config_file_path=r\"/kaggle/working/CVR16/configs/Baseline1.yml\",\n",
    "        project_root=r\"/kaggle/working/CVR16\"\n",
    "    )\n",
    "    trainer.train()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from datetime import datetime\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from models.GroupActivityClassifer import Group_Activity_Classifer\n",
    "from dataloader.DataLoader import Group, group_activity_labels , Person , person_activity_labels\n",
    "from eval_utils import get_f1_score, plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model\n",
    "\n",
    "class Person_Tranier:\n",
    "    def __init__(self , config_file_path, project_root):\n",
    "        super(Tranier , self).__init__()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.exp_dir = os.path.join(\n",
    "            f\"{self.Project_Root}/training/baseline3/{self.config['experiment']['output_dir']}\",\n",
    "            f\"{self.config['experiment']['name']}_V{self.config['experiment']['version']}_{timestamp}\"\n",
    "        )\n",
    "        self.model = Group_Activity_Classifer(num_classes=len(group_activity_labels)).to(self.device)\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "    def prepare_data(self):\n",
    "        new_train_transforms = A.Compose([\n",
    "            A.Resize(256, 256),  \n",
    "            A.RandomRotate90(),  \n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        new_val_transforms = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        new_train_dataset = Person(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['train'],\n",
    "            labels=person_activity_labels,\n",
    "            transform=new_train_transforms\n",
    "        )\n",
    "\n",
    "        new_val_dataset = Person(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['validation'],\n",
    "            labels=person_activity_labels,\n",
    "            transform=new_val_transforms\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"New training dataset size: {len(new_train_dataset)}\")\n",
    "        self.logger.info(f\"New validation dataset size: {len(new_val_dataset)}\")\n",
    "\n",
    "        new_train_loader = DataLoader(\n",
    "            new_train_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        new_val_loader = DataLoader(\n",
    "            new_val_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        return new_train_loader, new_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d871175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import albumentations as A\n",
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import  datetime \n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast , GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from models.PeronActivityClassifer import ClassiferNN\n",
    "from dataloader.DataLoader import Group, group_activity_labels , Person , person_activity_labels\n",
    "from eval_utils import get_f1_score, plot_confusion_matrix\n",
    "from helper_utils import load_config, setup_logging, save_checkpoint_model , load_checkpoint_model\n",
    "from baselines.trainer import Tranier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c095b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Training(Tranier):\n",
    "    def __init__(self , config_file_path , project_root , person_activity_checkpoints):\n",
    "        self.Project_Root = project_root\n",
    "        self.config = load_config(config_file_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.modela = Group_Activity_Classifer(num_classes=self.config.model['num_classes']['person_activity'])\n",
    "        self.person_activity = load_checkpoint_model(\n",
    "            checkpoint_path=person_activity_checkpoints,\n",
    "            model = self.modela ,\n",
    "            device=self.device, \n",
    "            optimizer=None\n",
    "            )\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.exp_dir = os.path.join(\n",
    "            f\"{self.Project_Root}/training/baseline3/{self.config['experiment']['output_dir']}\",\n",
    "            f\"{self.config['experiment']['name']}_V{self.config['experiment']['version']}_{timestamp}\"\n",
    "        )\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        self.set_seed(self.config['experiment']['seed'])\n",
    "        self.modelb = ClassiferNN(\n",
    "            person_feature_extraction=self.person_activity, \n",
    "        num_classes=self.config.model['num_classes']['group_activity']\n",
    "        )\n",
    "        self.optimizer = torch.optim.AdamW(self.modelb.parameters(),\n",
    "                        lr= self.config['training']['learning_rate'],\n",
    "                        weight_decay=self.config['training']['weight_decay'])\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=3, verbose=True ) \n",
    "        self.scaler = GradScaler()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.logger = setup_logging(self.exp_dir)\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(self.exp_dir, 'tensorboard'))\n",
    "        self.train_loader, self.val_loader = self.prepare_data()\n",
    "        self.class_names = self.config['model']['num_classes_label'][\"group_activity\"]\n",
    "        config_save_path = os.path.join(self.exp_dir, 'config.yaml')\n",
    "        with open(config_save_path, 'w') as config_file:\n",
    "            yaml.dump(self.config, config_file)\n",
    "        self.logger.info(f\"Configuration saved to {config_save_path}\")\n",
    "        super(Tranier , self).__init__()\n",
    "        \n",
    "    def concat(self , batch):\n",
    "            clips , labels =  zip(*batch)\n",
    "            max_box = 12\n",
    "            single_clips = []\n",
    "            single_labels = []\n",
    "            \n",
    "            for clip , label in zip(clips , labels):\n",
    "                num_boxes = clip.size(0)\n",
    "                if num_boxes < max_box:\n",
    "                    clip_padding = torch.zeros((max_box - num_boxes, clip.size(1), clip.size(2), clip.size(3)))\n",
    "                    clip = torch.cat((clip, clip_padding), dim=0)\n",
    "                \n",
    "                single_clips.append(clip)\n",
    "                single_labels.append(label)\n",
    "            \n",
    "            single_clips = torch.stack(single_clips)\n",
    "            single_labels = torch.stack(single_labels)\n",
    "            \n",
    "            return single_clips, single_labels\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        train_transforms = A.Compose([\n",
    "           A.Resize(224, 224),\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 7)),\n",
    "                A.ColorJitter(brightness=0.2),\n",
    "                A.RandomBrightnessContrast(),\n",
    "                A.GaussNoise()\n",
    "            ], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(),\n",
    "                A.VerticalFlip(),\n",
    "            ], p=0.05),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        val_transforms = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        train_dataset = Group(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['train'],\n",
    "            labels=group_activity_labels,\n",
    "            transform=train_transforms,\n",
    "            crops=True,\n",
    "            seq=False,\n",
    "        )\n",
    "\n",
    "        val_dataset = Group(\n",
    "            videos_path=self.config['data']['videos_path'],\n",
    "            annot_path=self.config['data']['annot_path'],\n",
    "            split=self.config['data']['video_splits']['validation'],\n",
    "            labels=group_activity_labels,\n",
    "            transform=val_transforms,\n",
    "            crops=True,\n",
    "            seq=False, \n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"New training dataset size: {len(train_dataset)}\")\n",
    "        self.logger.info(f\"New validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            collate_fn= self.concat,\n",
    "            shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            collate_fn= self.concat,\n",
    "            shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00b2ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output clips shape: torch.Size([3, 12, 3, 64, 64])\n",
      "Output labels shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# ✅ نجرب نعمل batch يدوي فيه 3 clips\n",
    "\n",
    "# clip1: 5 فريمات\n",
    "clip1 = torch.randn(5, 3, 64, 64)\n",
    "label1 = torch.tensor(0)\n",
    "\n",
    "# clip2: 12 فريم (مش هيحتاج padding)\n",
    "clip2 = torch.randn(12, 3, 64, 64)\n",
    "label2 = torch.tensor(1)\n",
    "\n",
    "# clip3: 8 فريمات\n",
    "clip3 = torch.randn(8, 3, 64, 64)\n",
    "label3 = torch.tensor(2)\n",
    "\n",
    "# نحطهم في batch\n",
    "batch = [\n",
    "    (clip1, label1),\n",
    "    (clip2, label2),\n",
    "    (clip3, label3)\n",
    "]\n",
    "\n",
    "# ✅ نجرب الفانكشن\n",
    "clips_out, labels_out = concat(batch)\n",
    "\n",
    "print(f\"Output clips shape: {clips_out.shape}\")\n",
    "print(f\"Output labels shape: {labels_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3554c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
